import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from pydantic import SecretStr
import re
import time

# åˆå§‹åŒ–OpenAIæ¨¡å‹
openai_model = ChatOpenAI(
    model='gpt-4o-mini',
    base_url='https://api.openai-hk.com/v1/',
    api_key=SecretStr('hk-oh9hmp1000056169583738a8eeccd1d6810502b65832f931'),
)

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings(
    model='embedding-3',
    api_key=SecretStr('96aefc8f6386478e9bd07014e413ee2b.garDGiiQT1QPDpCu'),
    base_url='https://open.bigmodel.cn/api/paas/v4'
)

# åˆå§‹åŒ–session state
if 'web_analysis_history' not in st.session_state:
    st.session_state.web_analysis_history = []
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'current_analysis' not in st.session_state:
    st.session_state.current_analysis = None


def crawl_web_page(url: str):
    """çˆ¬å–ç½‘é¡µå†…å®¹å¹¶æå–åŸºæœ¬ä¿¡æ¯"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        start_time = time.time()
        response = requests.get(url, headers=headers, timeout=10)
        response_time = time.time() - start_time
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
        meta_description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={
            'name': 'description'}) else ""

        headings = []
        for level in ['h1', 'h2', 'h3']:
            for heading in soup.find_all(level):
                headings.append({"text": heading.get_text().strip(), "level": level})

        paragraphs = [p.get_text().strip() for p in soup.find_all('p') if p.get_text().strip()]

        base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            if href.startswith('/'):
                full_url = urljoin(base_url, href)
            elif href.startswith('http'):
                full_url = href
            else:
                continue

            link_text = a.get_text().strip() or "æ— æ–‡æœ¬"
            links.append({"text": link_text[:50] + "..." if len(link_text) > 50 else link_text, "url": full_url})

        images = []
        for img in soup.find_all('img', src=True):
            src = img['src']
            if src.startswith('/'):
                full_src = urljoin(base_url, src)
            elif src.startswith('http'):
                full_src = src
            else:
                continue
            images.append({"alt": img.get('alt', 'æ— æè¿°æ–‡æœ¬'), "src": full_src})

        return {
            "url": url,
            "status": response.status_code,
            "response_time": round(response_time, 2),
            "title": title,
            "meta_description": meta_description,
            "headings": headings,
            "paragraphs": paragraphs,
            "links": links[:20],
            "images": images[:10],
            "html": response.text[:5000] + "..." if len(response.text) > 5000 else response.text,
            "error": None
        }

    except requests.exceptions.RequestException as e:
        return {"url": url, "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        return {"url": url, "error": f"åˆ†æç½‘é¡µæ—¶å‡ºé”™: {str(e)}"}


def ai_analyze_content(content: str, analysis_type: str = "summary"):
    """ä½¿ç”¨AIåˆ†æç½‘é¡µå†…å®¹"""
    try:
        if analysis_type == "summary":
            prompt = f"è¯·ä¸ºä»¥ä¸‹ç½‘é¡µå†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œçªå‡ºä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯:\n\n{content}"
        elif analysis_type == "sentiment":
            prompt = f"åˆ†æä»¥ä¸‹ç½‘é¡µå†…å®¹çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/ä¸­æ€§/æ¶ˆæï¼‰ï¼Œå¹¶è§£é‡ŠåŸå› :\n\n{content}"
        elif analysis_type == "keywords":
            prompt = f"ä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–5-10ä¸ªå…³é”®è¯ï¼Œå¹¶æŒ‰é‡è¦æ€§æ’åº:\n\n{content}"
        elif analysis_type == "qa":
            prompt = f"åŸºäºä»¥ä¸‹ç½‘é¡µå†…å®¹ï¼Œç”Ÿæˆ3ä¸ªæœ‰æ·±åº¦çš„é—®é¢˜åŠå…¶ç­”æ¡ˆ:\n\n{content}"
        else:
            prompt = f"è¯·åˆ†æä»¥ä¸‹ç½‘é¡µå†…å®¹:\n\n{content}"

        response = openai_model.invoke(prompt)
        return response.content
    except Exception as e:
        return f"AIåˆ†æå¤±è´¥: {str(e)}"


def create_vector_store_from_url(url: str):
    """ä»URLåˆ›å»ºå‘é‡å­˜å‚¨"""
    try:
        loader = WebBaseLoader(url)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " "]
        )
        splits = text_splitter.split_documents(docs)
        vector_store = FAISS.from_documents(splits, embeddings)
        return vector_store
    except Exception as e:
        st.error(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
        return None


def ask_question_about_web(question: str):
    """è¯¢é—®å…³äºç½‘é¡µå†…å®¹çš„é—®é¢˜"""
    if st.session_state.vector_store is None:
        return "è¯·å…ˆåˆ†æä¸€ä¸ªç½‘é¡µ"

    try:
        qa_chain = RetrievalQA.from_chain_type(
            llm=openai_model,
            chain_type="stuff",
            retriever=st.session_state.vector_store.as_retriever(),
            return_source_documents=True
        )

        prompt_template = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µå†…å®¹åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹ç½‘é¡µå†…å®¹å›ç­”é—®é¢˜:
        {context}

        é—®é¢˜: {question}
        è¯·ç»™å‡ºè¯¦ç»†ã€ä¸“ä¸šçš„å›ç­”:
        """
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        qa_chain.combine_documents_chain.llm_chain.prompt = PROMPT

        result = qa_chain({"query": question})
        return result["result"]
    except Exception as e:
        return f"æŸ¥è¯¢å¤±è´¥: {str(e)}"


# Streamlitç•Œé¢
st.set_page_config(page_title="æ™ºèƒ½ç½‘é¡µåˆ†æå·¥å…·", layout="wide")
st.title("ğŸŒ æ™ºèƒ½ç½‘é¡µåˆ†æå·¥å…·")
st.caption("è¾“å…¥ç½‘é¡µURLè¿›è¡Œåˆ†æï¼Œç„¶åå¯ä»¥ç›´æ¥æé—®å…³äºç½‘é¡µå†…å®¹çš„é—®é¢˜")

# URLè¾“å…¥å’Œåˆ†æéƒ¨åˆ†
url = st.text_input("è¯·è¾“å…¥ç½‘é¡µURL:", placeholder="https://example.com")

col1, col2 = st.columns(2)
with col1:
    analyze_btn = st.button("åˆ†æç½‘é¡µ", use_container_width=True)
with col2:
    clear_btn = st.button("æ¸…é™¤ç»“æœ", use_container_width=True)

if clear_btn:
    st.session_state.current_analysis = None
    st.session_state.vector_store = None
    st.rerun()

if analyze_btn and url:
    with st.spinner("æ­£åœ¨çˆ¬å–å’Œåˆ†æç½‘é¡µå†…å®¹..."):
        crawl_result = crawl_web_page(url)

        if "error" in crawl_result and crawl_result["error"]:
            st.error(crawl_result["error"])
        else:
            st.session_state.vector_store = create_vector_store_from_url(url)

            with st.spinner("ä½¿ç”¨AIåˆ†æç½‘é¡µå†…å®¹..."):
                content_text = "\n\n".join(crawl_result["paragraphs"][:10])
                st.session_state.current_analysis = {
                    **crawl_result,
                    "ai_summary": ai_analyze_content(content_text, "summary"),
                    "ai_sentiment": ai_analyze_content(content_text, "sentiment"),
                    "ai_keywords": ai_analyze_content(content_text, "keywords"),
                    "ai_qa": ai_analyze_content(content_text, "qa"),
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }

# æ˜¾ç¤ºåˆ†æç»“æœ
if st.session_state.current_analysis:
    analysis = st.session_state.current_analysis

    if st.button("ä¿å­˜åˆ†æç»“æœ", use_container_width=True):
        st.session_state.web_analysis_history.append(analysis)
        st.success("åˆ†æç»“æœå·²ä¿å­˜!")

    st.success(f"ç½‘é¡µåˆ†æå®Œæˆ! çŠ¶æ€ç : {analysis['status']}, å“åº”æ—¶é—´: {analysis['response_time']}ç§’")

    # åŸºæœ¬ä¿¡æ¯
    with st.expander("ğŸ“Œ åŸºæœ¬ä¿¡æ¯", expanded=True):
        st.subheader(analysis["title"])
        st.caption(analysis["url"])
        st.write(f"**Metaæè¿°:** {analysis['meta_description'] or 'æ— æè¿°'}")
        st.write(f"**AIæ‘˜è¦:** {analysis['ai_summary']}")

    # é—®ç­”éƒ¨åˆ†
    st.divider()
    st.subheader("ğŸ’¬ ç½‘é¡µå†…å®¹é—®ç­”")
    question = st.text_input("å…³äºç½‘é¡µå†…å®¹æœ‰ä»€ä¹ˆé—®é¢˜?", placeholder="ä¾‹å¦‚: è¿™ä¸ªç½‘é¡µçš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆ?")

    if st.button("æé—®", use_container_width=True) and question:
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            answer = ask_question_about_web(question)
            st.session_state.current_analysis["last_question"] = question
            st.session_state.current_analysis["last_answer"] = answer
            st.rerun()

    if "last_question" in st.session_state.current_analysis:
        st.markdown(f"**é—®é¢˜:** {st.session_state.current_analysis['last_question']}")
        st.markdown(f"**å›ç­”:** {st.session_state.current_analysis['last_answer']}")

    # è¯¦ç»†åˆ†æ
    st.divider()
    st.subheader("ğŸ” è¯¦ç»†åˆ†æ")

    with st.expander("ğŸ“Š å†…å®¹åˆ†æ", expanded=False):
        cols = st.columns(2)
        with cols[0]:
            st.write("**æƒ…æ„Ÿåˆ†æ:**")
            st.write(analysis["ai_sentiment"])
        with cols[1]:
            st.write("**å…³é”®è¯æå–:**")
            st.write(analysis["ai_keywords"])

    with st.expander("ğŸ“ ç”Ÿæˆé—®ç­”", expanded=False):
        st.write(analysis["ai_qa"])

    with st.expander("ğŸ“‘ å†…å®¹ç»“æ„", expanded=False):
        st.write("**æ ‡é¢˜ç»“æ„:**")
        for heading in analysis["headings"]:
            level = int(heading["level"][1])
            st.markdown(f"{'#' * level} {heading['text']}")

        st.write("**å†…å®¹æ®µè½:**")
        for i, para in enumerate(analysis["paragraphs"][:5]):
            st.write(f"{i + 1}. {para}")

    with st.expander("ğŸ”— é“¾æ¥å’Œèµ„æº", expanded=False):
        cols = st.columns(2)
        with cols[0]:
            st.write("**ç›¸å…³é“¾æ¥:**")
            for link in analysis["links"][:10]:
                st.markdown(f"- [{link['text']}]({link['url']})")
        with cols[1]:
            st.write("**å›¾ç‰‡èµ„æº:**")
            for img in analysis["images"][:3]:
                st.image(img["src"], caption=img["alt"], width=200)

    with st.expander("ğŸ“„ HTMLæºç é¢„è§ˆ", expanded=False):
        st.code(analysis["html"])

# ä¾§è¾¹æ å†å²è®°å½•
with st.sidebar:
    st.subheader("ğŸ“š å†å²è®°å½•")
    if st.session_state.web_analysis_history:
        for i, item in enumerate(reversed(st.session_state.web_analysis_history)):
            if st.sidebar.button(f"{i + 1}. {item['title'][:30]}...", key=f"hist_{i}"):
                st.session_state.current_analysis = item
                st.session_state.vector_store = create_vector_store_from_url(item["url"])
                st.rerun()
    else:
        st.sidebar.info("æš‚æ— å†å²è®°å½•")

    st.divider()
    st.caption("""
    **åŠŸèƒ½è¯´æ˜:**
    - ç½‘é¡µçˆ¬å–ä¸å†…å®¹æå–
    - AIå†…å®¹åˆ†æ(æ‘˜è¦/æƒ…æ„Ÿ/å…³é”®è¯)
    - åŸºäºå†…å®¹çš„æ™ºèƒ½é—®ç­”
    """)

# é¡µè„š
st.divider()
st.caption("Â©æ™ºèƒ½ç½‘é¡µåˆ†æå·¥å…· | ä½¿ç”¨GPT-4å’ŒLangChainæŠ€æœ¯")